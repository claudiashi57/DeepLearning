{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Claudia Shi\n",
    "#Js5334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries & dependencies for all three problems\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import L1L2\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse, loss, grad =  0.0 [0.0]\n"
     ]
    }
   ],
   "source": [
    "#Problem 1\n",
    "#This is the example given\n",
    "label = tf.constant(1.0, dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "\n",
    "loss_mse = tf.losses.mean_squared_error(label, x)\n",
    "gradient_mse = tf.gradients(loss_mse, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ci, gi = sess.run((loss_mse, gradient_mse), feed_dict={x: 1.0})\n",
    "    print(\"mse, loss, grad = \", ci, gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hinge loss, loss, grad =  0.0 [-0.0]\n"
     ]
    }
   ],
   "source": [
    "#Problem 1.1\n",
    "loss_hinge = tf.losses.hinge_loss(1,x)\n",
    "gradient_hinge = tf.gradients(loss_hinge, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    ci, gi = sess.run((loss_hinge, gradient_hinge), feed_dict={x: 1.001})\n",
    "    print(\"hinge loss, loss, grad = \", ci, gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hinge loss, loss, grad =  0.991 [-1.0]\n"
     ]
    }
   ],
   "source": [
    "#Problem 1.2 \n",
    "with tf.Session() as sess:\n",
    "    ci, gi = sess.run((loss_hinge, gradient_hinge), feed_dict={x: 0.009})\n",
    "    print(\"hinge loss, loss, grad = \", ci, gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c2651eb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Problem 1.3\n",
    "with tf.Session() as sess: \n",
    "    arr = np.linspace(-2,2,50)\n",
    "    ci_arr = []\n",
    "    gi_arr = []\n",
    "    for i in arr:\n",
    "        ci, gi = sess.run((loss_hinge, gradient_hinge), feed_dict={x: i})\n",
    "        ci_arr.append(ci)\n",
    "        gi_arr.append(gi)\n",
    "        #print(\"hinge loss, loss, grad = \", ci, gi)\n",
    "    plt.plot(arr,ci_arr,label=\"hinge loss\")\n",
    "    plt.plot(arr, gi_arr, label=\"loss\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2\n",
    "#Logistic regression and multi-layer perceptrons (MLPs) are two basic models for classification tasks. \n",
    "#Try to use these two models to learn from the following xor dataset:\n",
    "#Problem setup\n",
    "xs = np.array([[-1.1, 1.0], [-1.0, 1.1], [-1.1, 1.1], [1.0, -1.1],[1.1, -1.0],[1.0, -1.0],\n",
    "                  [1.1, 1.1],[1.0, 0.9],[1.1, 1.0],  [-1.1, -1.0], [-1.1, -1.1], [-1.0, -1.1]],\n",
    "                dtype=np.float32)\n",
    "ys = np.array([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0], dtype=np.float32)\n",
    "ys = ys[:, None]\n",
    "ys = tf.keras.utils.to_categorical(ys,2)\n",
    "split = 4 * len(ys) // 5\n",
    "\n",
    "xs = xs[:split]\n",
    "ys = ys[:split]\n",
    "xs_test = xs[split:]\n",
    "ys_test = ys[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def logistic_regression(xs, ys, xs_test, ys_test):\n",
    "#we have two classes \n",
    "    n_epochs = 50\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(2, input_dim=2, activation='sigmoid')\n",
    "            ])\n",
    "   \n",
    "    model.compile(optimizer='sgd', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(xs, ys, epochs=n_epochs)\n",
    "    model.evaluate(xs_test, ys_test)\n",
    "    print (model.predict(xs).round())\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 0s 23ms/step - loss: 0.7919 - acc: 0.5000\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 168us/step - loss: 0.7912 - acc: 0.5000\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 295us/step - loss: 0.7904 - acc: 0.5000\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 209us/step - loss: 0.7897 - acc: 0.5000\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 188us/step - loss: 0.7890 - acc: 0.5000\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 250us/step - loss: 0.7883 - acc: 0.5000\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 152us/step - loss: 0.7876 - acc: 0.5000\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 492us/step - loss: 0.7869 - acc: 0.5000\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 459us/step - loss: 0.7861 - acc: 0.5000\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 199us/step - loss: 0.7854 - acc: 0.5000\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 842us/step - loss: 0.7847 - acc: 0.5000\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 263us/step - loss: 0.7840 - acc: 0.5000\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 485us/step - loss: 0.7833 - acc: 0.5000\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 293us/step - loss: 0.7826 - acc: 0.5000\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 754us/step - loss: 0.7819 - acc: 0.5000\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 266us/step - loss: 0.7812 - acc: 0.5000\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 595us/step - loss: 0.7805 - acc: 0.5000\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 240us/step - loss: 0.7798 - acc: 0.5000\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 371us/step - loss: 0.7791 - acc: 0.5000\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 191us/step - loss: 0.7784 - acc: 0.5000\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 522us/step - loss: 0.7777 - acc: 0.5000\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 375us/step - loss: 0.7770 - acc: 0.5000\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 655us/step - loss: 0.7763 - acc: 0.5000\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 780us/step - loss: 0.7756 - acc: 0.5000\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 411us/step - loss: 0.7749 - acc: 0.5000\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 182us/step - loss: 0.7742 - acc: 0.5000\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 419us/step - loss: 0.7735 - acc: 0.5000\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 248us/step - loss: 0.7728 - acc: 0.5000\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 410us/step - loss: 0.7721 - acc: 0.5000\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 584us/step - loss: 0.7714 - acc: 0.5000\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 435us/step - loss: 0.7707 - acc: 0.5000\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 226us/step - loss: 0.7700 - acc: 0.5000\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 273us/step - loss: 0.7693 - acc: 0.5000\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 218us/step - loss: 0.7686 - acc: 0.5000\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 469us/step - loss: 0.7679 - acc: 0.5000\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 560us/step - loss: 0.7672 - acc: 0.5000\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 330us/step - loss: 0.7666 - acc: 0.5000\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 213us/step - loss: 0.7659 - acc: 0.5000\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 311us/step - loss: 0.7652 - acc: 0.5000\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 233us/step - loss: 0.7645 - acc: 0.5000\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 202us/step - loss: 0.7638 - acc: 0.5000\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 231us/step - loss: 0.7631 - acc: 0.5000\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 207us/step - loss: 0.7625 - acc: 0.5000\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 206us/step - loss: 0.7618 - acc: 0.5000\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 322us/step - loss: 0.7611 - acc: 0.5000\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 232us/step - loss: 0.7604 - acc: 0.5000\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 319us/step - loss: 0.7598 - acc: 0.5000\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 220us/step - loss: 0.7591 - acc: 0.5000\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 326us/step - loss: 0.7584 - acc: 0.5000\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 451us/step - loss: 0.7577 - acc: 0.5000\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(xs, ys, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(xs, ys, xs_test, ys_test):\n",
    "    n_hidden1 = 16\n",
    "    n_hidden2 = 16\n",
    "    n_epochs = 50\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(n_hidden1, input_dim=2),\n",
    "                tf.keras.layers.Dense(n_hidden2, input_dim=n_hidden1),\n",
    "                tf.keras.layers.Dense(2, input_dim=n_hidden2, activation='softmax')\n",
    "            ])\n",
    "   \n",
    "    model.compile(optimizer='sgd', \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(xs, ys, epochs=n_epochs)\n",
    "    model.evaluate(xs_test, ys_test)\n",
    "    print (model.predict(xs).round())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Multi-Layer Perceptron\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 0s 27ms/step - loss: 0.5442 - acc: 0.6667\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 332us/step - loss: 0.5349 - acc: 0.6667\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 274us/step - loss: 0.5258 - acc: 0.6667\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 299us/step - loss: 0.5171 - acc: 0.6667\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 253us/step - loss: 0.5086 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 221us/step - loss: 0.5004 - acc: 0.6667\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 210us/step - loss: 0.4925 - acc: 0.7778\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 351us/step - loss: 0.4847 - acc: 0.7778\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 248us/step - loss: 0.4772 - acc: 0.8889\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 241us/step - loss: 0.4699 - acc: 0.8889\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 260us/step - loss: 0.4628 - acc: 1.0000\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 209us/step - loss: 0.4558 - acc: 1.0000\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 252us/step - loss: 0.4491 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 226us/step - loss: 0.4425 - acc: 1.0000\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 303us/step - loss: 0.4360 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 253us/step - loss: 0.4297 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 249us/step - loss: 0.4236 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 231us/step - loss: 0.4176 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 248us/step - loss: 0.4117 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 299us/step - loss: 0.4060 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 194us/step - loss: 0.4003 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 214us/step - loss: 0.3948 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 230us/step - loss: 0.3894 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 210us/step - loss: 0.3841 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 205us/step - loss: 0.3789 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 199us/step - loss: 0.3739 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 220us/step - loss: 0.3689 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 224us/step - loss: 0.3640 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 258us/step - loss: 0.3592 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 226us/step - loss: 0.3544 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 189us/step - loss: 0.3498 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 245us/step - loss: 0.3453 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 296us/step - loss: 0.3408 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 287us/step - loss: 0.3364 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 380us/step - loss: 0.3321 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 240us/step - loss: 0.3278 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 241us/step - loss: 0.3236 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 240us/step - loss: 0.3195 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 304us/step - loss: 0.3155 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 276us/step - loss: 0.3115 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 290us/step - loss: 0.3076 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 215us/step - loss: 0.3037 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 284us/step - loss: 0.3000 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 328us/step - loss: 0.2962 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 315us/step - loss: 0.2926 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 483us/step - loss: 0.2890 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 232us/step - loss: 0.2854 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 238us/step - loss: 0.2819 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 426us/step - loss: 0.2785 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 237us/step - loss: 0.2751 - acc: 1.0000\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "mlp(xs, ys, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see, the logistic regression has its accuracy bounded at 0.5\n",
    "#This is not suprising because no linear separater can separate XOR\n",
    "#MLP on the other hand can solve this problem because it is not a linear separator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3\n",
    "#Now we write a function that would retrive the data. \n",
    "\n",
    "def get_data(val):\n",
    "    batch_size = 50000\n",
    "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "    xs, ys = mnist.train.next_batch(batch_size)\n",
    "\n",
    "    index = [index for index,y in enumerate(ys) if len(np.where((y==val)==False)[0])==0]\n",
    "    xs = xs[index]\n",
    "    ys = ys[index]\n",
    "    xs_test, ys_test = mnist.test.next_batch(batch_size)\n",
    "\n",
    "    \n",
    "    index_test = [index for index,y in enumerate(ys_test) if len(np.where((y==val)==False)[0])==0]\n",
    "    \n",
    "    xs_test = xs_test[index_test]\n",
    "    ys_test = ys_test[index_test]\n",
    "\n",
    "    return (xs, ys), (xs_test, ys_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_the_data():\n",
    "    four = [0.,0.,0.,1.,0.,0.,0.,0.,0.,0.]\n",
    "    eight = [0.,0.,0.,0.,0.,0.,0.,1.,0.,0.]\n",
    "    \n",
    "    (xs4,ys4),(xs_test4,ys_test4) = get_data(four)\n",
    "    (xs8,ys8),(xs_test8,ys_test8) = get_data(eight)\n",
    "    \n",
    "    xs = np.array(np.concatenate((xs4,xs8),axis=0))\n",
    "    ys = np.array(np.concatenate((ys4,ys8),axis=0))\n",
    "    xs_test = np.array(np.concatenate((xs_test4,xs_test8),axis=0))\n",
    "    ys_test = np.array(np.concatenate((ys_test4,ys_test8),axis=0))\n",
    "    \n",
    "    return (xs,ys),(xs_test,ys_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "(50000, 784)\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "(50000, 784)\n",
      "(5140, 784)\n"
     ]
    }
   ],
   "source": [
    "(xs,ys),(xs_test,ys_test) = reshape_the_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem One hidden layer MLP with cross entropy loss\n",
    "def MLP_CROSSENTROPY(xs, ys, xs_test, ys_test):\n",
    "    n_samples = xs.shape[0]\n",
    "    n_inputs = xs.shape[1]\n",
    "    n_hidden1 = 32\n",
    "    n_classes = ys.shape[1]\n",
    "    n_epochs = 20\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(n_hidden1, input_dim=n_inputs),\n",
    "                tf.keras.layers.Dense(n_classes, input_dim=n_hidden1, activation='softmax')\n",
    "            ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(), \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(xs, ys, epochs=n_epochs)\n",
    "    model.evaluate(xs_test, ys_test)\n",
    "    print(model.predict(xs).round())\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10331/10331 [==============================] - 1s 83us/step - loss: 0.0847 - acc: 0.9770\n",
      "Epoch 2/20\n",
      "10331/10331 [==============================] - 1s 64us/step - loss: 0.0368 - acc: 0.9891\n",
      "Epoch 3/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0347 - acc: 0.9896\n",
      "Epoch 4/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0326 - acc: 0.9905\n",
      "Epoch 5/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0314 - acc: 0.9902\n",
      "Epoch 6/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0302 - acc: 0.9918\n",
      "Epoch 7/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0294 - acc: 0.9908\n",
      "Epoch 8/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0286 - acc: 0.9911\n",
      "Epoch 9/20\n",
      "10331/10331 [==============================] - 1s 60us/step - loss: 0.0274 - acc: 0.9920\n",
      "Epoch 10/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0270 - acc: 0.9912\n",
      "Epoch 11/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0261 - acc: 0.9921\n",
      "Epoch 12/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0260 - acc: 0.9918\n",
      "Epoch 13/20\n",
      "10331/10331 [==============================] - 1s 56us/step - loss: 0.0266 - acc: 0.9918\n",
      "Epoch 14/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0251 - acc: 0.9921\n",
      "Epoch 15/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0251 - acc: 0.9924\n",
      "Epoch 16/20\n",
      "10331/10331 [==============================] - 1s 59us/step - loss: 0.0251 - acc: 0.9928\n",
      "Epoch 17/20\n",
      "10331/10331 [==============================] - 1s 59us/step - loss: 0.0229 - acc: 0.9934\n",
      "Epoch 18/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0238 - acc: 0.9930\n",
      "Epoch 19/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0243 - acc: 0.9925\n",
      "Epoch 20/20\n",
      "10331/10331 [==============================] - 1s 59us/step - loss: 0.0229 - acc: 0.9932\n",
      "4076/4076 [==============================] - 0s 43us/step\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "MLP_CROSSENTROPY(xs, ys, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem 2\n",
    "#One hidden layer MLP with hinge loss\n",
    "#same exact as the previous one, except \n",
    "def MLP_hingeloss(xs, ys, xs_test, ys_test):\n",
    "    n_samples = xs.shape[0]\n",
    "    n_inputs = xs.shape[1]\n",
    "    n_hidden1 = 32\n",
    "    n_classes = ys.shape[1]\n",
    "    n_epochs = 20\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(n_hidden1, input_dim=n_inputs),\n",
    "                tf.keras.layers.Dense(n_classes, input_dim=n_hidden1, activation='softmax')\n",
    "            ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(), \n",
    "                  loss='categorical_hinge',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(xs, ys, epochs=n_epochs)\n",
    "    model.evaluate(xs_test, ys_test)\n",
    "    print(model.predict(xs).round())\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10331/10331 [==============================] - 1s 111us/step - loss: 0.0621 - acc: 0.9802\n",
      "Epoch 2/20\n",
      "10331/10331 [==============================] - 1s 80us/step - loss: 0.0265 - acc: 0.9883\n",
      "Epoch 3/20\n",
      "10331/10331 [==============================] - 1s 66us/step - loss: 0.0234 - acc: 0.9893\n",
      "Epoch 4/20\n",
      "10331/10331 [==============================] - 1s 65us/step - loss: 0.0212 - acc: 0.9902\n",
      "Epoch 5/20\n",
      "10331/10331 [==============================] - 1s 63us/step - loss: 0.0204 - acc: 0.9911\n",
      "Epoch 6/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0180 - acc: 0.9916\n",
      "Epoch 7/20\n",
      "10331/10331 [==============================] - 1s 57us/step - loss: 0.0181 - acc: 0.9915\n",
      "Epoch 8/20\n",
      "10331/10331 [==============================] - 1s 58us/step - loss: 0.0165 - acc: 0.9924\n",
      "Epoch 9/20\n",
      "10331/10331 [==============================] - 1s 74us/step - loss: 0.0162 - acc: 0.9923\n",
      "Epoch 10/20\n",
      "10331/10331 [==============================] - 1s 65us/step - loss: 0.0150 - acc: 0.9930\n",
      "Epoch 11/20\n",
      "10331/10331 [==============================] - 1s 85us/step - loss: 0.0150 - acc: 0.9927\n",
      "Epoch 12/20\n",
      "10331/10331 [==============================] - 1s 63us/step - loss: 0.0144 - acc: 0.9931\n",
      "Epoch 13/20\n",
      "10331/10331 [==============================] - 1s 59us/step - loss: 0.0139 - acc: 0.9934\n",
      "Epoch 14/20\n",
      "10331/10331 [==============================] - 1s 64us/step - loss: 0.0147 - acc: 0.9927\n",
      "Epoch 15/20\n",
      "10331/10331 [==============================] - 1s 65us/step - loss: 0.0135 - acc: 0.9934\n",
      "Epoch 16/20\n",
      "10331/10331 [==============================] - 1s 69us/step - loss: 0.0129 - acc: 0.9940\n",
      "Epoch 17/20\n",
      "10331/10331 [==============================] - 1s 69us/step - loss: 0.0134 - acc: 0.9935\n",
      "Epoch 18/20\n",
      "10331/10331 [==============================] - 1s 64us/step - loss: 0.0129 - acc: 0.9938\n",
      "Epoch 19/20\n",
      "10331/10331 [==============================] - 1s 61us/step - loss: 0.0117 - acc: 0.9943\n",
      "Epoch 20/20\n",
      "10331/10331 [==============================] - 1s 73us/step - loss: 0.0121 - acc: 0.9942\n",
      "4076/4076 [==============================] - 0s 64us/step\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "MLP_hingeloss(xs, ys, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we compare these two models. We see that they both perform exceeding well.\n",
    "#Hinge loss performed slightly better than crossentropy loss. \n",
    "#could be that hinge loss better model the problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bonus problem\n",
    "#MLP with two and three hidden layers\n",
    "def MLP_Multilayer(xs, ys, xs_test, ys_test):\n",
    "    n_samples = xs.shape[0]\n",
    "    n_inputs = xs.shape[1]\n",
    "    n_hidden1 = 32\n",
    "    n_hidden2 = 32\n",
    "    n_hidden3 = 8\n",
    "    n_classes = ys.shape[1]\n",
    "    n_epochs = 20\n",
    "    \n",
    "    model = tf.keras.models.Sequential([\n",
    "                tf.keras.layers.Dense(n_hidden1, input_dim=n_inputs),\n",
    "                tf.keras.layers.Dense(n_hidden2, input_dim= n_hidden1),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Dense(n_hidden3, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.2),  \n",
    "                tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "            ])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(), \n",
    "                  loss='categorical_hinge',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(xs, ys, epochs=n_epochs)\n",
    "    model.evaluate(xs_test, ys_test)\n",
    "    print(model.predict(xs).round())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10331/10331 [==============================] - 1s 126us/step - loss: 0.1443 - acc: 0.9441\n",
      "Epoch 2/20\n",
      "10331/10331 [==============================] - 1s 89us/step - loss: 0.0524 - acc: 0.9835\n",
      "Epoch 3/20\n",
      "10331/10331 [==============================] - 1s 110us/step - loss: 0.0489 - acc: 0.9860\n",
      "Epoch 4/20\n",
      "10331/10331 [==============================] - 1s 89us/step - loss: 0.0436 - acc: 0.9871\n",
      "Epoch 5/20\n",
      "10331/10331 [==============================] - 1s 83us/step - loss: 0.0384 - acc: 0.9895\n",
      "Epoch 6/20\n",
      "10331/10331 [==============================] - 1s 81us/step - loss: 0.0360 - acc: 0.9894\n",
      "Epoch 7/20\n",
      "10331/10331 [==============================] - 1s 86us/step - loss: 0.0329 - acc: 0.9894\n",
      "Epoch 8/20\n",
      "10331/10331 [==============================] - 1s 75us/step - loss: 0.0305 - acc: 0.9897\n",
      "Epoch 9/20\n",
      "10331/10331 [==============================] - 1s 73us/step - loss: 0.0285 - acc: 0.9894\n",
      "Epoch 10/20\n",
      "10331/10331 [==============================] - 1s 72us/step - loss: 0.0261 - acc: 0.9906\n",
      "Epoch 11/20\n",
      "10331/10331 [==============================] - 1s 76us/step - loss: 0.0232 - acc: 0.9913\n",
      "Epoch 12/20\n",
      "10331/10331 [==============================] - 1s 113us/step - loss: 0.0215 - acc: 0.9911\n",
      "Epoch 13/20\n",
      "10331/10331 [==============================] - 1s 73us/step - loss: 0.0206 - acc: 0.9912\n",
      "Epoch 14/20\n",
      "10331/10331 [==============================] - 1s 92us/step - loss: 0.0209 - acc: 0.9907: 0s - loss: 0.01\n",
      "Epoch 15/20\n",
      "10331/10331 [==============================] - 1s 103us/step - loss: 0.0187 - acc: 0.9917\n",
      "Epoch 16/20\n",
      "10331/10331 [==============================] - 1s 89us/step - loss: 0.0176 - acc: 0.9919\n",
      "Epoch 17/20\n",
      "10331/10331 [==============================] - 1s 61us/step - loss: 0.0184 - acc: 0.9912\n",
      "Epoch 18/20\n",
      "10331/10331 [==============================] - 1s 65us/step - loss: 0.0183 - acc: 0.9911\n",
      "Epoch 19/20\n",
      "10331/10331 [==============================] - 1s 67us/step - loss: 0.0187 - acc: 0.9908\n",
      "Epoch 20/20\n",
      "10331/10331 [==============================] - 1s 66us/step - loss: 0.0176 - acc: 0.9915\n",
      "4076/4076 [==============================] - 0s 70us/step\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "MLP_Multilayer(xs, ys, xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is interesting to observe that despite having a much more complex structure. \n",
    "#The model performed not as well as the two simplex models earlier. \n",
    "#This could be a result from overfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
